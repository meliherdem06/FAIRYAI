# FAIRYAI - Client-Side AI Chatbot

A fully client-side AI chatbot web application powered by WebLLM and the Mistral-7B-Instruct model. FAIRYAI runs entirely in the browser using WebGPU with no server or external API required.

## âœ¨ Features

- ğŸ¤– **Client-Side AI**: Runs completely in the browser using WebLLM
- ğŸ“ **File Upload**: Upload .txt files for context-aware responses
- ğŸŒ™ **Dark Mode**: Beautiful dark/light theme toggle
- ğŸ“± **Responsive Design**: Works perfectly on desktop and mobile
- âš¡ **Fast**: Powered by WebGPU for optimal performance
- ğŸ”’ **Privacy-First**: No data leaves your device

## ğŸš€ Live Demo

Visit the live application: [FAIRYAI on GitHub Pages](https://yourusername.github.io/FAIRYAI/)

## ğŸ› ï¸ Technology Stack

- **Frontend**: React 18 + TypeScript
- **Build Tool**: Vite
- **Styling**: TailwindCSS with dark mode
- **AI Engine**: @mlc-ai/web-llm
- **Model**: Mistral-7B-Instruct-q4f16_1
- **Deployment**: GitHub Pages

## ğŸ“‹ Prerequisites

- Node.js 18+ 
- Modern browser with WebGPU support (Chrome 113+, Edge 113+, Firefox Nightly)
- Git

## ğŸš€ Quick Start

### 1. Clone the Repository

```bash
git clone https://github.com/yourusername/FAIRYAI.git
cd FAIRYAI
```

### 2. Install Dependencies

```bash
npm install
```

### 3. Start Development Server

```bash
npm run dev
```

The application will open at `http://localhost:3000`

### 4. Build for Production

```bash
npm run build
```

### 5. Preview Production Build

```bash
npm run preview
```

## ğŸ“– Usage

### Basic Chat
1. Wait for the AI model to load (Mistral-7B-Instruct)
2. Type your question in the chat input
3. Press Enter or click the send button
4. View the AI response

### Context-Aware Chat
1. Upload a .txt file using the file upload area
2. The file content will be loaded as context
3. Ask questions about the uploaded content
4. FAIRYAI will provide responses based on the file context

### Dark Mode
- Click the sun/moon icon in the header to toggle dark mode
- Your preference is automatically saved

## ğŸ—ï¸ Project Structure

```
FAIRYAI/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”œâ”€â”€ ChatInterface.tsx    # Main chat component
â”‚   â”‚   â”œâ”€â”€ FileUpload.tsx       # File upload component
â”‚   â”‚   â””â”€â”€ Header.tsx           # Header with dark mode toggle
â”‚   â”œâ”€â”€ types.ts                 # TypeScript type definitions
â”‚   â”œâ”€â”€ App.tsx                  # Main application component
â”‚   â”œâ”€â”€ main.tsx                 # React entry point
â”‚   â””â”€â”€ index.css                # Global styles with TailwindCSS
â”œâ”€â”€ public/                      # Static assets
â”œâ”€â”€ package.json                 # Dependencies and scripts
â”œâ”€â”€ vite.config.ts              # Vite configuration
â”œâ”€â”€ tailwind.config.js          # TailwindCSS configuration
â”œâ”€â”€ tsconfig.json               # TypeScript configuration
â””â”€â”€ README.md                   # This file
```

## ğŸš€ Deployment

### GitHub Pages (Automatic)

The project is configured for automatic deployment to GitHub Pages:

1. Push your code to the main branch
2. Run the deployment script:
   ```bash
   npm run deploy
   ```
3. The app will be deployed to `https://yourusername.github.io/FAIRYAI/`

### Manual Deployment

1. Build the project:
   ```bash
   npm run build
   ```
2. Deploy the `dist` folder to your web server

## ğŸ”§ Configuration

### Vite Configuration
The `vite.config.ts` file is configured with:
- React plugin
- Base path for GitHub Pages (`/FAIRYAI/`)
- Development server settings

### TailwindCSS Configuration
The `tailwind.config.js` includes:
- Dark mode support
- Custom color palette
- Responsive design utilities

## ğŸ¤ Contributing

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- [MLC-LLM](https://github.com/mlc-ai/mlc-llm) for the WebLLM framework
- [Mistral AI](https://mistral.ai/) for the Mistral-7B-Instruct model
- [Vite](https://vitejs.dev/) for the fast build tool
- [TailwindCSS](https://tailwindcss.com/) for the utility-first CSS framework

## ğŸ› Troubleshooting

### Model Loading Issues
- Ensure your browser supports WebGPU
- Check that you have sufficient memory (4GB+ recommended)
- Try refreshing the page if the model fails to load

### Performance Issues
- Close other browser tabs to free up memory
- Use a modern browser with WebGPU support
- Consider using a device with dedicated graphics

### File Upload Issues
- Ensure the file is a .txt format
- Check that the file size is reasonable (< 10MB recommended)
- Try refreshing the page if upload fails

## ğŸ“ Support

If you encounter any issues or have questions:
1. Check the troubleshooting section above
2. Search existing issues on GitHub
3. Create a new issue with detailed information

---

Made with â¤ï¸ using React, TypeScript, and WebLLM 